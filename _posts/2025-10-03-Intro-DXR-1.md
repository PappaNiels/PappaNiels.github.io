---
layout: post
title:  "Intro to DirectX Raytracing: The concepts (part 1)"
date:   2025-10-03 09:00:13 +0100
categories: posts
permalink: /posts/intro-dxr/part-1
---

Computer graphics is evolving rapidly. More techniques are applied, optimizations are created and we got more computational power as ever before. Everything to make the final image look as realistic as possible (or not). In 2018, Nvidia announced their new graphics card with 'special' RT cores. This meant, that shipping games with ray traced and more realistic graphics was possible, all while it was smooth and enough for gamers to enjoy those beautiful graphics.

The following articles will show my interpretation on what DirectX Raytracing is and how it works. After reading these articles, you would be able to create a ray tracer using DXR, and you are able to expand it with new ray tracing concepts or resources. In a later article I will also show you how you can use inline

<figure>
    <img src="../../assets/dxr/part-1/voxel-example.jpg"
         alt="Ray traced voxels"
         height=388
         width=680>
    <figcaption><i>Ray traced voxels using a CPU ray tracer.</i></figcaption>
</figure>

***

# Table of content
- [Basics](#Basics)
    - [Vector Math](#Vector-Math)
    - [Ray Generation](#Ray-Generation)
    - [Ray Intersection](#Ray-Intersection)
    - [Acceleration Structures](#Acceleration-Structures)
- [The DirectX Raytracing API](#DirectX-Raytracing)
    - [Acceleration Structures in DXR](#Acceleration-Structures-DXR)
    - [Shaders](#Shaders)
        - [Ray Generation Shader](#RayGen-Shader)
        - [Closest Hit Shader](#ClosestHit-Shader)
        - [Miss Shader](#Miss-Shader)
        - [Intersection Shader](#Intersection-Shader)
        - [Any Hit Shader](#AnyHit-Shader)
        - [Raytracing Pipeline](#Raytracing-Pipeline)
- [References](#References)

<span id="Basics">
# Basics

<span id="Vector-Math">
### Vector Math
But what is ray tracing exactly? Ray tracing is a simulation of light transportation, which simulates the lighting better than the before used method for computer graphics,rasterization. The main downside, performance, as it is not a method that is really great for the architecture of the computer as we know it today. Luckily, the GPU manufacturers are pushing the boundaries of GPU performance, making sure we can use ray tracing as our method of rendering graphics, rather than it remaining in movies or other applications.

So, how do we trace our rays? Lets start with the basics and the most scary part for a lot of people. **Math!**. Do not worry. I will try to make it as simple as possible, so a train monkey can trace rays using the math examples I will provide.

Ray tracing mainly consists of vector math. A vector consists of multiple numbers representing either a position or a direction within a coordinate system. For our case, we will have a vector of three numbers for our 3D world. Mathematicians would note a 3D vector as the following:

$$\begin{pmatrix}
x \\ y \\ z
\end{pmatrix}$$

A ray consists of at least two vectors. One is for the direction the ray goes, and one for the origin. As last, a ray has a number (or lambda) that is used as the distance the ray has traveled. 

$$x = \vec{O} + \vec{D} * t$$

> Note:
> The direction of the ray needs to be normalized (magnitude needs to be 1.0) to be to make sure we do not go through and skip our geometry. We can do that by dividing the vector by the length of the vector. 

$$\vec{t}_n = \dfrac{\vec{t}}{||\vec{t}||}$$

<i>Normalizing a vector.</i>
<br>

Lets do a simple example. Lets say, we are at the coordinate (2.5, -1.0, 0.0) and we look towards (0.5, 0.5, 0.0). At a distance of 3.0, there is an object. If we use the before mentioned formula, we can calculate the location of that object.

$$x = \vec{O} + \vec{D} * t$$

$$ \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 2.5 \\ -1.0 \\ 0.0 \end{pmatrix} + \begin{pmatrix} 0.5 \\ 0.5 \\ 0.0 \end{pmatrix} \times 3.0 $$

$$ = \begin{pmatrix} 2.5 \\ -1.0 \\ 0.0 \end{pmatrix} + \begin{pmatrix} 1.5 \\ 1.5 \\ 0.0 \end{pmatrix}$$

$$ = \begin{pmatrix} 4.0 \\ 0.5 \\ 0.0 \end{pmatrix} $$

Now we know where the object is in the world ((4.0, 0.5, 0.0) in this case) and we can do more calculations based on that position (e.g. lighting, shadows, reflections, ...). If we take that to C/C++ code it would look like this:

```cpp
struct Ray
{
    float3 Origin;      // O
    float3 Direction;   // D
    float  Distance;    // t
};

float3 GetWorldPosition(const Ray& ray)
{
    return ray.Origin + ray.Direction * ray.Distance;
}
``` 
<span id="Ray-Generation">
### Ray Generation
The creation, or so called 'generation' of the rays is very simple. A ray gets generated by having the camera's position as the origin and the direction that goes through a pixel of the render target. This can be one or more, if that is desired. 

This can be done as seen below in pseudo code (HLSL equivalent by Microsoft in their <a href="https://github.com/microsoft/DirectX-Graphics-Samples/blob/master/Samples/Desktop/D3D12Raytracing/src/D3D12RaytracingSimpleLighting/Raytracing.hlsl#L79-L93" target="_blank">sample</a>). This takes the projection into account, so the user can change it easily by just changing the matrices.

```cpp
Ray GetPrimaryRay(const float3& cameraPosition, uint2 index, const float4x4& inverseVP) 
{
    Ray ray = {};
    ray.Origin = cameraPosition;

    // Set the position on the middle of the pixel
    float2 screenPos = float2(index) + float2(0.5f);

    // Transform from [0..ScreenDimension] to [0..1]
    screenPos = screenPos / RenderTargetDimensions;

    // Transform from [0..1] to [-1..1]
    screenPos = screenPos * 2.0f - 1.0f;

    // Depending on the graphics API. For DXR, it is needed...
    screenPos.y = 1.0f - screenPos.y; 

    // Transform from clip to world space
    float4 world = inverseVP * float4(screenPos, 0.0f, 1.0f);

    // Perspective divide
    world.xyz / world.z;

    // Create ray direction
    ray.Direction = normalize(world.xyz - cameraPosition);

    return ray;
}
``` 
<br>

<figure>
    <img src="../../assets/dxr/part-1/ray-gen-visualisation.png"
         alt="Ray generation visualisation"
         height=333
         width=354>
    <figcaption><i>Ray generation visualization, where E is the origin, and h the render target's dimensions. [<a href="https://en.wikipedia.org/wiki/Ray_tracing_%28graphics%29" target="_blank">Source</a>]</i></figcaption>
</figure>

<span id="Ray-Intersection">
### Ray Intersection

So we know where our camera is in the scene, the direction and where an object is with all its properties. But we are missing one key aspect. We do not know what the closest object is to the camera that the ray intersects with. That is where the intersection functions come into play. These functions calculate if the ray intersects with the object and at what distance. Lets start with a simple intersection function. The one from a sphere.

$$\text{$r^2 = x^2 + y^2 + z^2,\:$ where $\:r^2 < x^2 + y^2 + z^2 \:$ is inside of the sphere}$$

After rewriting the function to something more useful for us (what Peter Shirley did in [Ray tracing in one weekend](https://raytracing.github.io/books/RayTracingInOneWeekend.html#addingasphere/ray-sphereintersection)), you would get a mathematical function that looks like this:

$$D\cdot D \times t^2 - 2 \times t \times D \cdot (C - O) + (C - O) \cdot (C - O) - r^2 = 0$$

$$D$$ is the ray direction, $$O$$ is the ray origin, $$C$$ is the center of the sphere and $$r$$ is the radius of the sphere. The only variable that is unknown and is what we are solving for is $$t$$ (the ray distance). After filling the parameters inside of this function, it looks like a quadratic equation, which can easily be solved using the so called 'quadratic formula'.

$$\frac{-b \pm \sqrt{b^2 - 4 \times a \times c}}{2 \times a}$$

Lets look at the discriminant of the quadratic equation. Because there is a possibility of early out there where we can save some resources. If the discriminant is less than zero, then the ray missed the sphere entirely, making solving for $$t$$ unnecessary. If the discriminant is zero or greater than zero, we have a hit and we can proceed doing all the calculations needed to solve for $$t$$. Since we want the closest object to the origin, we only solve for $$t$$ using the minus version of the quadratic formula. 

<figure>
    <img src="../../assets/dxr/part-1/sphere-intersection.png"
         alt="Sphere intersection"
         height=395
         width=405>
    <figcaption><i>Sphere intersection visualized, where D is the discriminant.</i></figcaption>
</figure>

<span id="Acceleration-Structures">
### Acceleration Structures

todo :)

So this is ray tracing in a nutshell. We create rays from the camera, we trace them, test them against objects and return the colour closest object to the camera to create a render target, which we present to the screen. 

If you want to know more about vectors and vector math, you can watch [this](https://www.youtube.com/watch?v=Ej3ZVxljJfo) YouTube video from FloatyMonkey to fully understand vector math. If you want to explore more of the basics of ray tracing, check out [Ray tracing in one weekend](https://raytracing.github.io/books/RayTracingInOneWeekend.html) by Peter Shirley to learn more!

<span id="DirectX-Raytracing">
# DirectX Raytracing: API

<span id="Acceleration-Structures-DXR">
### Acceleration Structures in DXR

<span id="Shaders">
### Shaders
With this API, comes a new set of shaders. As ray tracing replaces rasterization, we will have to say goodbye to our old vertex, pixel, and other shaders, and we will bring new shaders to replace them and to do our job. Unlike the rasterization and compute shaders, these shaders are compiled as a library, rather than e.g. a vertex shader. More on that, in the next part.
These are the ray tracing shaders:

<span id="RayGenShader">
#### Ray Generation Shader
The ray generation shader is the start of the pipeline. This shader allows you to create the rays that will be traced with your set settings. After you have 'generated' your ray, you trace the ray, which traverses the BVH and call the other shaders internally. 
The main attributes you need to have for this shader is a TLAS, an unordered access view as a 2D texture and optionally camera data.

```hlsl
RaytracingAccelerationStructure g_Scene : register(t0);
ConstantBuffer<Camera> g_Camera : register(b0);
RWTexture2D<float4> g_RenderTarget : register(u0);
```
<figcaption><i>Attributes used in the ray generation shader.</i></figcaption><br>

Because you can generate the rays yourself, you can do way more than just rendering to the render target.

<span id="ClosestHitShader">
#### Closest Hit Shader

<span id="MissShader">
#### Miss Shader

<span id="IntersectionShader">
#### Intersection Shader

<span id="AnyHitShader">
#### Any Hit Shader

<span id="Raytracing-Pipeline">
#### Raytracing Pipeline

<span id="References">
# References

<span id="Ref-1">
- [1] <a target="_blank" href="https://raytracing.github.io/books/RayTracingInOneWeekend.html">Ray Tracing In One Weekend</a>, Peter Shirley, 2018 <br>
- [2] <a target="_blank" href="https://jacco.ompf2.com/2022/04/13/how-to-build-a-bvh-part-1-basics/">BVH Building</a>, Jacco Bikker, 2022<br>
- [3] <a target="_blank" href="https://microsoft.github.io/DirectX-Specs/d3d/Raytracing.html">DirectX Specs</a>, Microsoft, 2017<br>
- [4] <a target="_blank" href="https://github.com/microsoft/DirectX-Graphics-Samples">DirectX Samples</a>, Microsoft <br>